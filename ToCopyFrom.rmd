---
editor_options: 
  markdown: 
    wrap: 72
---

# Regression

In this tutorial, we are going to look at THREE versions of a model to
assess whether people who make over \$75000 are more likely to live in
places with higher population density e.g. "downtown". We are also going
to look to see if the residuals (what is left over) from our model has
spatial autocorrelation (bad).

### Getting the data

This tutorial builds on the example from the LISA tutorial. So IF YOU
ARE DOING LAB 6 YOU HAVE ALREADY DONE THIS AND MOVE ONTO "BASICS"

<br>

<details>

<summary>**Code to load the example data**</summary>

<br>

We can load your data from the census and tidy up, before plotting

FOR THIS CODE CHUNK, set results='hide' as a code-chunk option, or it
will swamp your knit: {r, results='hide'}

```{r, eval=FALSE, results="hide", message=FALSE,warning-FALSE}
# THIS WILL NOT WORK IF YOU DIDN'T RUN YOUR API KEY CODE & RESTART R-STUDIO LAST LAB


# Download Illinois ACS data at census tract level for my chosen variables, using tidycensus
IL.Census.sf <- get_acs(
   geography = "tract",
   year = 2017,
   variables = c(
      total_pop        = "B05012_001", # total population
      total_house      = "B25001_001", #total housing units
      income.gt75      = "B06010_011",# number of people making > 75000 USD
      med.income       = "B19013_001", #median income
      total.foreignppl = "B05012_003", #number of foreign born people
      total.BAdegree   = "B15012_001" , #total with at least a bachelors degree
      total.workhome   = "B08101_049", #number who work from home#total housing units
      house.mean_age   = "B25035_001", #average house age
      house.mean_beds  = "B25041_001", #total homes number of beds in the house
      housetotal.owneroccupied = "B25003_002", #total homes owner occupied
      housetotal.broadband  = "B28002_004", #total homes with broadband access
      housetotal.noplumbing = "B25047_003"), #total homes without plumbing
   state = "IL",
   survey = "acs5",
   geometry = TRUE,
   output = "wide",
   cache_table = TRUE)
```

<br>

We also need to change the map projection. As we're focusing on a single
city, lets use a local UTM projection, which has EPSG code 26916.

To find one for your area, FIRST find your city's UTM zone, then google which EPSG code you need for it. (the EPSG code is the shortcut number to tell R what projection we are using) 

-   <https://mangomap.com/robertyoung/maps/69585/what-utm-zone-am-i-in->
-   There's also a great tutorial here on choosing the best map projection
<https://source.opennews.org/articles/choosing-right-map-projection/>

```{r, eval=FALSE}
# Change the map projection to Albers equal projection, 
# then remove empty polygons (lakes etc) and fix any broken geometry
IL.Census.sf <- IL.Census.sf %>%
                st_transform(26916) %>%
                st_make_valid()
```

<br>

You can see the data summarized here. Each column ending in E means the
*estimate* of the value in each census tract and the column ending in M
means the *margin of error*.

```{r}
head(IL.Census.sf)
```

For more advanced work we probably want to keep the error columns. But
here let's remove them.

```{r, eval=FALSE}
# Remove and rename error columns 
IL.Census.sf <- IL.Census.sf %>%
   dplyr::select(
      GEOID, 
      NAME,
      total_pop        = total_popE, 
      total_house      = total_houseE,
      income.gt75      = income.gt75E,
      med.income       = med.incomeE, 
      total.foreignppl = total.foreignpplE,
      total.BAdegree   = total.BAdegreeE,             
      total.workhome   = total.workhomeE,
      house.mean_age   = house.mean_ageE,
      house.mean_beds  = house.mean_bedsE,
      housetotal.owneroccupied = housetotal.owneroccupiedE,
      housetotal.broadband     = housetotal.broadbandE,
      housetotal.noplumbing    = housetotal.noplumbingE,
      geometry )
```

<br>

Rather than looking at the total population, it makes sense to scale by
the size of each tract, so that bigger tracts don't skew our results. We
want:

-   the population density (e.g. total_pop / Area)<br>
-   the percentage of people earning over \$75K (e.g. income.gt75 /
    total_pop)<br>

<br>

```{r, eval=FALSE}

# Find the areas in each county & change the units from metres squared to km squared
IL.Census.sf$Area  <- st_area(IL.Census.sf)
IL.Census.sf$Area  <- as.numeric(set_units(IL.Census.sf$Area,"km^2"))


# Now work out percentage ppl and homes rather than totals

IL.Census.sf <- IL.Census.sf %>% 
  mutate(
    pop.density_km2    = total_pop   / Area,
    house.density_km2  = total_house / Area,
    percent.income.gt75 = income.gt75 / total_pop,
    percent.foreignppl = total.foreignppl / total_pop,
    percent.BAdegree   = total.BAdegree   / total_pop,
    percent.workhome   = total.workhome   / total_pop,
    housepercent.owneroccupied = housetotal.owneroccupied / total_house,
    housepercent.broadband     = housetotal.broadband     / total_house,
    housepercent.noplumbing    = housetotal.noplumbing    / total_house
  )


# Finally select the final columns we want
IL.Census.sf <- IL.Census.sf %>%
   dplyr::select(
      GEOID, NAME, Area, 
      total_pop, pop.density_km2, 
      total_house, house.density_km2,
      income.gt75, percent.income.gt75,
      med.income, house.mean_age, 
      percent.foreignppl, percent.BAdegree, 
      percent.workhome, housepercent.broadband,
      housepercent.owneroccupied, housepercent.noplumbing)
      

```

<br>

```{r, include=FALSE}
IL.Census.sf <- IL.Census.tmp

```

Now let's crop to the Chicago metropolitan area

```{r, results="hide", message=FALSE, warning=FALSE}

# Get the Chicago city limits
cb.sf            <- core_based_statistical_areas(cb = TRUE, year=2017)
Chicago.city.sf  <- filter(cb.sf, grepl("Chicago", NAME))


# and set the projection to be identical to the census data
Chicago.city.sf  <- Chicago.city.sf %>%
                    st_transform(26916) %>%
                    st_make_valid()


# subset the Illinois census data with the Chicago city limits
Chicago.Census.sf <- ms_clip(target = IL.Census.sf, 
                             clip = Chicago.city.sf, 
                             remove_slivers = TRUE)

```

<br>

Now let's remove any empty fields

```{r}
# remove missing data and/or census tracts with no ppl in
Chicago.Census.sf <- Chicago.Census.sf %>% 
                     filter(pop.density_km2 > 0) %>% 
                     filter(!st_is_empty(geometry)) %>% 
                     st_make_valid()





```

and we're up to date!

</details>

::: small-gap
:::

<br> <br>

\\

## Regression

If none of the summary below makes sense, this is a great overview

-   <https://towardsdatascience.com/linear-regression-explained-1b36f97b7572/>

\\

Regression helps us understand how one variable changes in relation to
another. A simple regression line shows the general trend between two
variables (the "line of best fit"), and gives us a way to describe that
trend using an equation:

$$
\hat{y} = b_0 + b_1 x
$$

\\

Here,

-   $\hat{y}$ is the predicted value of our *response variable, y* for
    any given value of our *predictor variable, x*. The little hat on
    top means "predicted/estimated"

-   $b_0$ is the intercept - the value of the line when it crosses ZERO
    x.

-   $b_1$ refers to the slope or gradient of the line. e.g. how much
    does y increase for a given x.

Finally, we know that value of y for every object in our study isn't
going to end up *exactly* on the line. So the value of y for an
individual object (called i) can be written as the predicted value plus
the [residual]{.underline} (the amount left over), $\varepsilon_i$ :

$$
y_i = \hat{y_i} + \varepsilon_i
$$

\\

![Basic regression example. Each dot is an individual
object.](index_images/pg_364Lab7_TutRegression_BasicCartoon.png)

\\ \\

## Example 1 - Simple Linear Regression

Building on the LISA tutorial, suppose we want to understand whether the
percentage of people earning more than \$75,000 in each census tract in
Chicago is influenced by the area's population density.

-   **Object of analysis**: A single census tract

-   **'Strict' population:** All census tracts within Chicago city
    limits during the year 2020.

-   **'Inferrable' population**: Depends on context, but probably
    anywhere in a US metropolitan area in the last decade or so?

-   **Response variable, y (the thing we are trying to predict) :**
    Percentage of people earning more than \$75000 in each census tract.

-   **Predictor variable, x :** Population density of each census tract
    (number of people per km^2^).

\\ \\

[***Note***]{.underline} *- I really want to explore whether people who
make over \$75000 are more likely to live in places with higher
population density (say "downtown").*

*But we don't have data for individual people!*

*ALL WE CAN TEST IS: Do census tracts which contain a higher population
density ALSO contain a higher percentage of people who make \> \$75000..
Mixing these up is called the ecological fallacy*.

\\ \\

```{r}

# Filter to a new data frame with only numeric columns
house.numeric.columns <- Chicago.Census.sf[ , sapply(Chicago.Census.sf,is.numeric)]
house.numeric.columns <- na.omit(st_drop_geometry(house.numeric.columns))


cor_vector <- cor(house.numeric.columns)["percent.income.gt75", , drop = FALSE]

corrplot(cor_vector, method = "number",
         addshade="all", cl.pos="n",
         is.corr = TRUE, number.cex =.7)

corrplot(cor(house.numeric.columns),method="ellipse",type="lower")


```

### Initial Scatterplot

First, let's have a look at the scatterplot. We can make a basic
scatterplot using the plot command. But just as easily, we can make an
interactive plot using ggplot and plotly.

If you want some tutorials on customising these plots see here

-   <http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually>

```{r}

p <- Chicago.Census.sf %>% 
  ggplot(aes(
    x = pop.density_km2, 
    y = percent.income.gt75, 
    label = NAME
  )) +
  geom_point(size = 1, alpha = 0.5) +
  labs(
    x = "Census tract population density (per km²)",
    y = "% households per tract with income > $75k USD"
  ) +
  theme_bw(base_size = 10)

ggplotly(p)
```

<br>

#### Deal with the outlier

Our results are dominated by one *huge* outlier.. Click on it and you
will see that it's census tract 307.02! (mouse over it). The easiest way
I could think to look at it was to just look at any row where the
population density was over 150000 ppl per km2.

```{r}
Chicago.Census.sf[Chicago.Census.sf$pop.density_km2 > 150000,]
```

<br>

OK so the issue is with Census Tract 307.02 in Cook County.

I then googled this census tract name, which took me here
<https://censusreporter.org/profiles/14000US17031030702-census-tract-30702-cook-il/>

It looks like some apartments?? So I googled the location and looked at
streetview to find this:

[![*The entire census tract is one very tall set of apartment
blocks!*](index_images/pg_364Lab7_Regression_2021_fig7.png)](https://maps.app.goo.gl/8La5EVVRjMskJmZ88)

<br>

The entire census tract is one set of high-rise apartments, so *of
course* this census tract has a ridiculously high population density,
there are thousands of people living in a tiny area.

In that case, the next question is what do we do? We could:

-   Remove it and say it's "bad data".

-   Remove it and treat it as a special case

-   Leave it in.

The first option is inappropriate because there's nothing wrong with the
data. However, I think it would be reasonable to remove this census
tract and study it separately as a special case. My reasoning is because
it's unlikely the data is independent. It's only one or two buildings,
and they're likely to be full of apartments that are all likely to be
tailored to the same financial demographic of people.

So I shall remove it

```{r}
# I want to keep everything LESS than 150000
Chicago.Census.sf <- Chicago.Census.sf[Chicago.Census.sf$pop.density_km2  < 150000,] 
```

<br>

#### Deal with units

The second minor issue is that

TO COMPLETE

#### Replot

This gives some improvement, but not much!

```{r}
# Make an interactive plot
p <- Chicago.Census.sf %>% 
  ggplot(aes(
    x = pop.density_km2, 
    y = percent.income.gt75, 
    label = NAME
  )) +
  geom_point(size = 1, alpha = 0.5) +
  labs(
    x = "Census tract population density (per km²)",
    y = "% households per tract with income > $75k USD"
  ) +
  theme_bw(base_size = 10)

ggplotly(p)

```

OK, we can see that there is *possibly* a positive relationship between
the two, but a lot of spread.

<br>

### Fitting a simple linear regression model 

Now let's make a linear fit using the OLS package. To do this, use the
lm command, then y \~ x where y and x are the column names of your
response and predictor variables. na.exclude means ignore any missing
values.

![](index_images/pg_364Lab7_Regression_2021_fig8.png)

```{r,echo=FALSE}

# lm( y ~ x, data=tablename)

fit1.lm <- lm(percent.income.gt75 ~ pop.density_km2, 
              data = Chicago.Census.sf,
              na.action="na.exclude")


```

We can use this output to write out the regression equation using latex
maths symbols.

TO COMPLETE

So from the above, our model is now:<br>

```{r, L7Eqn1, echo=FALSE}
knitr::include_graphics('./index_images/pg_364Lab7_Regression_2021_eqn1.png')
```

### Checking if our model is any good.

Let's look at the summary:

<br>

```{r}
summary(fit1.lm)

```

Our model is now: percentage.workfromhome = 0.008+ 0.127xper.income.gt75

We can add a fit using abline (or check out R graph gallery):

```{r}
plot(Chicago.Census.sf$per.income.gt75E  ~ Chicago.Census.sf$pop.densityE,pch=16,cex=.5,col="blue")
abline(fit1.lm)
```

<br>

### Mapping the results

So we have a model! Let's look at the results spatially. I'm switching
tmap mode to plot to save computer power, just switch it back to view
for interactive maps.

```{r}
tmap_mode("plot")
```

```{r}
map_actual<- tm_shape(Chicago.Census.sf, unit = "mi") + 
  tm_polygons(col = "per.income.gt75E", style = "quantile",
              palette = "-Spectral", border.alpha = 0, 
              title = "ACTUAL: %ppl making > $75K")+
  tm_shape(Chicago.city.sf) +
  tm_borders()+
  tm_layout(legend.outside = TRUE)

map_actual
```

```{r}
Chicago.Census.sf$Fit1.lm.Prediction <- predict(fit1.lm)

# subset the Illinois census data with the Chicago city limits
maplm1 <- tm_shape(Chicago.Census.sf, unit = "mi") + 
  tm_polygons(col = "Fit1.lm.Prediction", style = "quantile",
              palette = "-Spectral", border.alpha = 0, 
              title = "MODEL1 prediction: %ppl making > $75K")+
  tm_shape(Chicago.city.sf) +
  tm_borders()+
  tm_layout(legend.outside = TRUE) 

maplm1
```

There is some indication of pattern, but it's not great.

<br> <br>

#### Looking for Spatial autocorrelation in residuals

One of the 4 assumptions around regression is that your data should be
independent.

If that is true then our *residuals* (what is left over) should not have
any influence or knowledge of each other. We know that census data is
highly geographic, so let's look at a MAP of the residuals (e.g. the
distance from each point to the model line of best fit, high means the
model underestimated the data, negative means the model overestimated
the data).

To do, this we first add the residuals to our table

```{r}
Chicago.Census.sf$Fit1.Residuals <- residuals(fit1.lm)
```

Now let's have a look! If we have fully explained all the data and the
data is spatially independent then there should be no pattern.

We can look at the residuals directly.

```{r}
# subset the Illinois census data with the Chicago city limits


tm_shape(Chicago.Census.sf, unit = "mi") + 
           tm_polygons(col = "Fit1.Residuals", style = "quantile",
                       palette = "-RdBu", border.alpha = 0, 
                       title = "Fit 1 residuals.Raw")+
    tm_shape(Chicago.city.sf) +
           tm_borders()+
    tm_layout(legend.outside = TRUE) 


```

Ouch - but maybe they are not significantly different and we just got
unlucky with our colour scale. Let's look at the extreme residuals by
converting to standard deviations.

```{r}
Chicago.Census.sf$sd_breaks <- scale(Chicago.Census.sf$Fit1.Residuals)[,1]
# because scale is made for matrices, we just need to get the first column using [,1]

my_breaks <- c(-14,-3,-2,-1,1,2,3,14)


tm_shape(Chicago.Census.sf) + 
  tm_fill("sd_breaks", title = "Fit1 Residuals (StandardDev)", style = "fixed", breaks = my_breaks, palette = "-RdBu") +
  tm_borders(alpha = 0.1) +
  tm_layout(main.title = "Fit1 Residuals - units = standard deviations from 0", main.title.size = 0.7 ,
            legend.position = c("right", "bottom"), legend.title.size = 0.8)+
      tm_layout(legend.outside = TRUE) 

```

A good model should have very few residuals that are more than 2
standard deviations from zero and no patterns. This doesn't look good...

<br>

To see if we are "seeing pictures in clouds" or there really is a
spatial pattern, we can look at a Moran's scatterplot with a queen's
spatial matrix. The test confirms highly significant autocorrelation.
Our p-values and regression model coefficients cannot be trusted.

<br>

```{r}
spatial.matrix.queen <-poly2nb(Chicago.Census.sf, queen=T)

weights.queen <- nb2listw(spatial.matrix.queen, style='B',zero.policy=TRUE)

moran.plot(Chicago.Census.sf$Fit1.Residuals, weights.queen,
           xlab = "Model 1 residuals",
           ylab = "Neighbors residuals",zero.policy=TRUE)
```

```{r}
moran.test(Chicago.Census.sf$Fit1.Residuals, weights.queen,zero.policy=TRUE)
```

We have a problem! Our residuals definitely do not look like independent
random noise - in fact there is a HUGE cluster of high residuals near
the centre and other residuals around the edge. They also look highly
significant.

The real life interpretation is that in downtown Chicago, the model is
underestimating the percentage of people making more than \$75K - and in
the suburbs it is underestimating.

Let's try a better model:

<br> <br>

### Model 2: Multiple regression

Our model isn't very good. Let's try multiple predictors - so this is
Multiple Linear Regression. I'm going to hope that my spatial dependence
can be removed by including some confounding variables.

<br>

So, now lets see if adding a second predictor variable makes the fit
better. I'm guessing that census tracts where there are more people
earning over \$75K might also be more likely to own their own home.

```{r}
# Make an interactive plot
# http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually

p <- Chicago.Census.sf %>%                  
  ggplot( aes(pop.densityE,per.income.gt75E,col= per.owneroccupiedE,label=NAME)) +
  geom_point() +
  theme_classic()+
  scale_color_gradient(low="blue", high="red")

ggplotly(p)
```

<br>

There seems to be some relationship. Let's ADD this to the model and
take a look. We now explain a little more..

```{r}
# using the OLS package
fit2.lm <- lm(per.income.gt75E ~ pop.densityE + per.owneroccupiedE, data = Chicago.Census.sf)
```

We can see the model equation here, it's harder to see on a scatterplot

```{r}
fit2.lm
```

```{r, include=FALSE, results=FALSE}
equatiomatic::extract_eq(fit2.lm,use_coefs=TRUE)
```

So from the above, our model is now:<br>

```{r, L7Eqn2, echo=FALSE}
knitr::include_graphics('./index_images/pg_364Lab7_Regression_2021_eqn2.png')
```

<br> <br>

#### Is model 2 any good?

Look at the summary

```{r}
ols_regress(fit2.lm)
```

and examine our prediction

```{r}
Chicago.Census.sf$Fit2.lm.Prediction <- predict(fit2.lm)

# subset the Illinois census data with the Chicago city limits
maplm2 <- tm_shape(Chicago.Census.sf, unit = "mi") + 
  tm_polygons(col = "Fit2.lm.Prediction", style = "quantile",
              palette = "-Spectral", border.alpha = 0, 
              title = "MODEL2 prediction: %ppl making > $75K")+
  tm_shape(Chicago.city.sf) +
  tm_borders()+
  tm_layout(legend.outside = TRUE) 
```

```{r, message=FALSE,warning=FALSE}
map_actual

maplm2
```

Also, the model improved! A little! We can see that the R^2^ went up to
0.188. But it's still pretty rubbish.

<br><br>

#### Looking for Spatial autocorrelation in Model 2 residuals

How about the spatial autocorrelation?

To do, this we first add the new residuals to our table

```{r}
Chicago.Census.sf$Fit2.Residuals <- residuals(fit2.lm)
```

Now let's have a look! If we have fully explained all the data and the
data is spatially independent then there should be no pattern.

We can look at the residuals directly:

```{r}
# subset the Illinois census data with the Chicago city limits
tm_shape(Chicago.Census.sf, unit = "mi") + 
           tm_polygons(col = "Fit2.Residuals", style = "quantile",
                       palette = "-RdBu", border.alpha = 0, 
                       title = "Fit 2 residuals: Raw values")+
    tm_shape(Chicago.city.sf) +
           tm_borders()+
    tm_layout(legend.outside = TRUE) 
```

Or.. we can look at the extreme residuals by converting to standard
deviation.

```{r}
Chicago.Census.sf$sd_breaks <- scale(Chicago.Census.sf$Fit2.Residuals)[,1]
# because scale is made for matrices, we just need to get the first column using [,1]

my_breaks <- c(-14,-3,-2,-1,1,2,3,14)

tm_shape(Chicago.Census.sf) + 
  tm_fill("sd_breaks", title = "Fit2 Residuals (StandardDev)", style = "fixed", breaks = my_breaks, palette = "-RdBu") +
  tm_borders(alpha = 0.1) +
  tm_layout(main.title = "Fit 2 Residuals (Standard Deviation away from 0)", main.title.size = 0.7 ,
            legend.position = c("right", "bottom"), legend.title.size = 0.8)+
      tm_layout(legend.outside = TRUE) 

```

<br>

We still have a problem! It is definitely not independent random noise -
there are still HUGE clusters of high residuals near the centre and
other residuals around the edge. E.g. in downtown Chicago, the model is
underestimating the percentage of people making more than \$75K - and in
the suburbs it is underestimating.

To see if we are "seeing pictures in clouds" or there really is a
spatial pattern, we can look at a Moran's scatterplot with a queen's
spatial matrix. The test confirms highly significant autocorrelation.
Our p-values and regression model coefficients cannot be trusted.

<br>

```{r}
spatial.matrix.queen <-poly2nb(Chicago.Census.sf, queen=T)

weights.queen <- nb2listw(spatial.matrix.queen, style='B',zero.policy=TRUE)

moran.plot(Chicago.Census.sf$Fit2.Residuals, weights.queen,
           xlab = "Model residuals",
           ylab = "Neighbors residuals",zero.policy=TRUE)
```

```{r}
moran.test(Chicago.Census.sf$Fit2.Residuals, weights.queen,zero.policy=TRUE)
```

So there is still highly significant spatial autocorrelation.

### Comparing models 1 and 2

How do we know if the "improvement" we saw from model 2 is enough to be
significant or meaningful?

One way to check is to compare the two models using ANOVA. This conducts
a hypothesis test to assess whether there is additional value to adding
a new variable.

```{r}
anova(fit1.lm,fit2.lm)
```

In this case, the p-value is very low - so it would be very unusual to
see this much benefit by chance if per.owneroccupiedE wasn't useful.
E.g, it might be good to keep per.owneroccupiedE.

<br>

We can also use AIC a non parametric statistic, where the LOWER number
is typically the better fit (taking into account over-fitting). In this
case, we can see that it thinks per.owneroccupiedE is a useful variable
to keep even if it has limited influence.

```{r}
AIC(fit1.lm,fit2.lm)
```

<br> <br>

### Model 3: Spatial lag model - OPTIONAL, here for information and your projects

Both models 1 and 2 have not been spatially independent and really
should not be used as our final prediction.

<br>

If the Moran's test is significant (as in this case), then we possibly
need to think of a more suitable model to represent our data: a spatial
regression model. Remember spatial dependence means that (more
typically) there will be areas of spatial clustering for the residuals
in our regression model. We want a better model that does not display
any spatial clustering in the residuals.

<br>

There are two general ways of incorporating spatial dependence in a
regression model:

-   A spatial error model
-   A spatial lagged model

<br>

The difference between these two models is both technical and
conceptual. The spatial error model assumes that the:

*“Spatial dependence observed in our data does not reflect a truly
spatial process, but merely the geographical clustering of the sources
of the behavior of interest. For example, citizens in adjoining
neighborhoods may favour the same (political) candidate not because they
talk to their neighbors, but because citizens with similar incomes tend
to cluster geographically, and income also predicts vote choice. Such
spatial dependence can be termed attributional dependence” (Darmofal,
2015: 4)*

<br>

The spatially lagged model, on the other hand, incorporates spatial
dependence explicitly by adding a “spatially lagged” variable y on the
right hand side of our regression equation. It assumes that spatial
processes THEMSELVES are an important thing to model:

*“If behavior is likely to be highly social in nature, and understanding
the interactions between interdependent units is critical to
understanding the behavior in question. For example, citizens may
discuss politics across adjoining neighbors such that an increase in
support for a candidate in one neighborhood directly leads to an
increase in support for the candidate in adjoining neighborhoods”
(Darmofal, 2015: 4)*

<br>

Mathematically, it makes sense to run both models and see which fits
best. We can do this using the `lm.LMtests()` function. (note, we are
skipping over complexity here!). See here for more details on the full
process:
<https://maczokni.github.io/crimemapping_textbook_bookdown/spatial-regression-models.html>

<br>

But that goes beyond the scope of this course. Here we will try the
spatial lag model, because I can imagine that things like broadband
access have explicit spatial relationships (e.g. where the cable goes)

To fit a spatial lag model, we use

```{r}
fit_2_lag <- lagsarlm(per.income.gt75E ~  pop.densityE + per.owneroccupiedE, data = Chicago.Census.sf, weights.queen,zero.policy=TRUE)
fit_2_lag
```

<br>

This is now going beyond the scope of this course, instead of a simple
linear model, we are running a generalized additive model (GAM), which
is mathematically more complex:

per.income.gt75 = rho(SPATIAL.WEIGHTS\*per.income.gt75) + b0 +
b1xpop.densityE + b2xper.owneroccupiedE e.g.

per.income.gt75 = 0.11255(SPATIAL.WEIGHTS\*per.income.gt75) + -0.0358 +
0.0000026054xpop.densityE + 0.121449xper.owneroccupiedE

<br>

You will notice that there is a new term Rho. What is this? This is our
spatial lag. It is a variable that measures the percentage working from
home in the census tracts SURROUNDING each tract of interest in our
spatial weight matrix.

<br>

We are simply using this variable as an additional explanatory variable
to our model, so that we can appropriately take into account the spatial
clustering detected by our Moran’s I test. You will notice that the
estimated coefficient for this term is both positive and statistically
significant. In other words, when the percentage working from home in
surrounding areas increases, so does the percentage working from home in
each country, even when we adjust for the other explanatory variables in
our model.

<br> <br>

#### Compare model fit

Let's use AIC to compare all 3 models (ANOVA is inappropriate for a GAM)

```{r}
AIC(fit1.lm,fit2.lm,fit_2_lag)
```

We see that our new lagged version has the lowest AIC and so is likely
to be the best model (so far) for predicting the percentage of people
who work from home in each census tract.

<br>

#### Check autocorrelation

Now, if we have fully taken into account the spatial autocorrelation of
our data, the spatial residuals should show less pattern and less
autocorrelation. Let's take a look,

<br>

```{r}
# Create the residuals
Chicago.Census.sf$Fit2.LaggedResiduals <- residuals(fit_2_lag)

Chicago.Census.sf$sd_breaks.lagged <- scale(Chicago.Census.sf$Fit2.LaggedResiduals)[,1]
# because scale is made for matrices, we just need to get the first column using [,1]

#plot standard deviations
my_breaks <- c(-14,-3,-2,-1,1,2,3,14)
tm_shape(Chicago.Census.sf) + 
  tm_fill("sd_breaks", title = "Fit2Lag Residuals (StandardDev)", style = "fixed", breaks = my_breaks, palette = "-RdBu",midpoint=0) +
  tm_borders(alpha = 0.1) +
  tm_layout(main.title = "Residuals for lagged model (Standard Deviation away from 0)", main.title.size = 0.7 ,
            legend.position = c("right", "bottom"), legend.title.size = 0.8)+
      tm_layout(legend.outside = TRUE)
```

```{r}
#plot Moran's I
moran.plot(Chicago.Census.sf$Fit2.LaggedResiduals, weights.queen,
           xlab = "Model residuals",
           ylab = "Neighbors residuals",zero.policy=TRUE)
```

```{r}
moran.test(Chicago.Census.sf$Fit2.LaggedResiduals, weights.queen,zero.policy=TRUE)


```

Well... we have improvement, but there is still significant spatial
variability left in the model. This suggests that I guessed completely
the wrong process for what influences the percentage of people making
over \$75K in a census tract. In this case, I would go away and think
then come back with a new model (amount of green space? proximity to
schools or the metro?).

We could also consider adjusting our spatial weights matrix (maybe a
"neighbor" is a 2nd order queens, or the census tracks within 50km..).

But even so, we have successfully included location in our model.

<br>

#### Plotting the results

So we have a model!

Finally, we went to all the trouble to create a model predicting the
percentage of people who make more than \$75K - let's look at all the
results together:

```{r}
Chicago.Census.sf$Fit2.Lagged.Prediction <- predict(fit_2_lag)

# subset the Illinois census data with the Chicago city limits
maplm2LAG <- tm_shape(Chicago.Census.sf, unit = "mi") + 
  tm_polygons(col = "Fit2.Lagged.Prediction", style = "quantile",
              palette = "-Spectral", border.alpha = 0, 
              title = "MODEL2LAG prediction: %ppl making > $75K")+
  tm_shape(Chicago.city.sf) +
  tm_borders()+
  tm_layout(legend.outside = TRUE) 
```

```{r, message=FALSE,warning=FALSE}

tmap_arrange(map_actual,maplm1,maplm2,maplm2LAG)

```

Still not great, but better.... and we could continue to improve given
that this model is missing something that pushes down numbers in the
suburbs (maybe distance to facilities?).

<br>

This process is the centre of an entire course I teach on regression if
anyone is interested in the topic:
<https://online.stat.psu.edu/stat462/>

## Challenge D2:

8.  **Step 8:** <br> OPTIONAL. Make a new sub-heading called Regression.
    Below, answer these questions. As in the previous challenges, make
    sure they are clearly marked to make it easy to find and grade them.
    NOTE these will be covered in later lectures (see here also: )

<!-- -->

a.  What are the 4 assumptions underpinning linear regression?
    (<https://www.statology.org/linear-regression-assumptions/>)<br>
b.  If the data is spatially auto-correlated, which assumption is
    broken?<br>
c.  What is the ecological fallacy and why is it important when looking
    at census data <br>
d.  If I tested 3 models using AIC and saw the values (Model1: -2403,
    Model2: -2746, Model3:-3102), which model would I be likely to
    choose? <br>

<br>

9.  **Step 9:** <br> Now for your city. Conduct a SIMPLE LINEAR
    REGRESSION analysis to assess whether the percentage of people who
    work from home is linked to the percentage of people who have access
    to broadband. <br> Make sure to tell me what you are doing at each
    stage and to interpret your output in text. Note, we will also be
    going through this in lectures.

<br>

10. **Step 10:** <br> Now for your city. Conduct a MULTIPLE LINEAR
    REGRESSION analysis to assess whether the percentage of people who
    work from home is linked to the percentage of people who have access
    to broadband AND some other variable you think might influence it.
    <br> Make sure to tell me what you are doing at each stage and to
    interpret your output in text.

<br>

11. **Step 11:** <br> Compare models 1 and 2 using the techniques in the
    tutorial

<br>

12. **Step 12:** <br> The American Community Survey data was collected
    in 2017. Do you think these results are stable now? If not, why not?

<br>

13. **Step 13:** <br> OPTIONAL.<br> For 4 show me something new points,
    go further. Apply and interpret either/or/both multiple regression
    (e.g. two or more things that might influence the percentage of
    people who work from home), AND/OR apply and interpret a spatial lag
    model. I will also consider exceptional analyses for course credit.

<br>

## E. Above and beyond

Remember that an A is 93%, so you can ignore this section and still
easily get an A. But here is your time to shine. Also, if you are
struggling in another part of the lab, you can use this to gain back
points.

**To get the final 4 marks in the lab, you need to show me something
new, e.g. you need to go above and beyond the lab questions in some
way.**

-   You get 2/4 for doing something new in any way
-   You get 4/4 for something really impressive (see the two ideas in
    the steps)

Please tell us in your R script what you did!

<br><br>

## F. Submitting your Lab

Remember to save your work throughout and to spell check your writing
(left of the knit button). Now, press the knit button again. If you have
not made any mistakes in the code then R should create a html file in
your lab 6 folder which includes your answers. If you look at your lab 6
folder, you should see this there - complete with a very recent
time-stamp.

In that folder, double click on the html file. This will open it in your
browser. CHECK THAT THIS IS WHAT YOU WANT TO SUBMIT

Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 6.

<br><br>

## Lab 6 submission check-list

**For all answers: Full marks = everything down at a high standard, in
full sentences as appropriate with no parts of your answer missing.
Imagine it as an example I use in class**

**HTML FILE SUBMISSION - 5 marks**

**RMD CODE SUBMISSION - 5 marks**

**MARKDOWN STYLE - 10 MARKS**

We will start by awarding full marks and dock marks for mistakes.LOOK AT
YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT

TO GET 13/13 : All the below PLUS your equations use the equation format
& you use subscripts/superscript as appropriate

TO GET 12/13 - all the below:

-   Your document is neat and easy to read.
-   Answers are easy to find and paragraphs are clear (e.g. you left
    spaces between lines)
-   You have written in full sentences, it is clear what your answers
    are referring to.
-   You have used the spell check.
-   You have used YAML code to make your work look professional (themes,
    tables of contents etc)

**Above and beyond: 4 MARKS**

-   You get 2/4 for doing something new in any way
-   You get 4/4 for something really impressive

An easy example, try conducting a Moran's analysis for your census data.
Bonus marks if you can convert a column to TRUE/FALSE and do a join
counts analysis on it! [100 marks total]

Overall, here is what your lab should correspond to:

```{r, echo=FALSE}
rubric <- readxl::read_excel("pg_364Lab_rubrictable.xlsx")
knitr::kable(rubric) %>%   
  kable_classic_2() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```

::: {style="margin-bottom:25px;"}
:::

------------------------------------------------------------------------

Website created and maintained by [Helen
Greatrex](https://www.geog.psu.edu/directory/helen-greatrex). Website
template by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)
