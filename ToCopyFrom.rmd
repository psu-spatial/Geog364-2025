---
editor_options: 
  markdown: 
    wrap: 72
---

### Mapping the results

#### Add model output to our data

So we have a model! The scatterplot looks pretty good, but given that
each object is an individual census tract, we can also make some maps.
The easiest way to do this is to add the model forecast values to our
data.frame.

```{r}
Chicago.Census.sf$fit1_predict  <-  predict(fit1.lm)
Chicago.Census.sf$fit1_residual <-  residuals(fit1.lm)
```

<br><br>

### Mapping the results

So we have a model! The scatterplot looks pretty good, but given that
each object is an individual census tract, we can also make some maps.

These maps can show the actual values of x and y, the *predicted* values
of x and y and the difference between them (the residuals).

For these maps, I'm switching tmap mode to plot to save computer power,
but switch it back to view any time to see the interactive maps.

```{r}
tmap_mode("plot")
```

```{r}


map_actual_y <- tm_shape(Chicago.Census.sf, unit = "km") +
  tm_polygons(
    fill = "percent.income.gt75",
    lwd = 0,
    fill.scale = tm_scale_intervals(values = "viridis",style="jenks")) +
  tm_shape(st_geometry(Chicago.city.sf), unit = "km") +
  tm_borders()+
  tm_title("ACTUAL % ppl per tract making > $75K", size = 0.95) +
  tm_layout(
    legend.outside = TRUE,
    frame = FALSE
  )

map_predicted_y <- tm_shape(Chicago.Census.sf, unit = "km") +
  tm_polygons(
    fill = "fit1_predict",
    lwd = 0,
    fill.scale = tm_scale_intervals(values = "viridis",style="jenks")) +
  tm_shape(st_geometry(Chicago.city.sf), unit = "km") +
  tm_borders()+
  tm_title("MODEL1 prediction: % ppl per tract making > $75K", size = 0.95) +
  tm_layout(
    legend.outside = TRUE,
    frame = FALSE
  )

map_residuals <- tm_shape(Chicago.Census.sf, unit = "km") +
  tm_polygons(
    fill = "fit1_residual",
    lwd = 0,
    fill.scale = tm_scale_intervals(values = "brewer.prgn",midpoint=0)) +
  tm_shape(st_geometry(Chicago.city.sf), unit = "km") +
  tm_borders()+
  tm_title("MODEL1 RESIDUALS (difference between actual and prediction)", size = 0.95) +
  tm_layout(
    legend.outside = TRUE,
    frame = FALSE
  )


suppressMessages(tmap_arrange(map_actual_y,map_predicted_y,map_residuals))

```

<br>

So here we can see that the model is doing a reasonable job - the
residuals are within 0.2 to +0.2 (e.g. we're overestimating the % people
making \>\$75K in the purple areas by around 0.2% and underestimating by
about the same in the green areas.

<br><br>

#### Looking for Spatial autocorrelation in residuals

One of the 4 assumptions around regression is that your data should be
independent.

If that is true then our *residuals* (what is left over) should not have
any influence or knowledge of each other. We know that census data is
highly geographic, so let's look at a MAP of the residuals (e.g. the
distance from each point to the model line of best fit, high means the
model underestimated the data, negative means the model overestimated
the data).

To do, this we first add the residuals to our table

```{r}
Chicago.Census.sf$Fit1.Residuals <- residuals(fit1.lm)
```

Now let's have a look! If we have fully explained all the data and the
data is spatially independent then there should be no pattern.

We can look at the residuals directly.

```{r}
# subset the Illinois census data with the Chicago city limits


tm_shape(Chicago.Census.sf, unit = "mi") + 
           tm_polygons(col = "Fit1.Residuals", style = "quantile",
                       palette = "-RdBu", border.alpha = 0, 
                       title = "Fit 1 residuals.Raw")+
    tm_shape(Chicago.city.sf) +
           tm_borders()+
    tm_layout(legend.outside = TRUE) 


```

Ouch - but maybe they are not significantly different and we just got
unlucky with our colour scale. Let's look at the extreme residuals by
converting to standard deviations.

```{r}
Chicago.Census.sf$sd_breaks <- scale(Chicago.Census.sf$Fit1.Residuals)[,1]
# because scale is made for matrices, we just need to get the first column using [,1]

my_breaks <- c(-14,-3,-2,-1,1,2,3,14)


tm_shape(Chicago.Census.sf) + 
  tm_fill("sd_breaks", title = "Fit1 Residuals (StandardDev)", style = "fixed", breaks = my_breaks, palette = "-RdBu") +
  tm_borders(alpha = 0.1) +
  tm_layout(main.title = "Fit1 Residuals - units = standard deviations from 0", main.title.size = 0.7 ,
            legend.position = c("right", "bottom"), legend.title.size = 0.8)+
      tm_layout(legend.outside = TRUE) 

```

A good model should have very few residuals that are more than 2
standard deviations from zero and no patterns. This doesn't look good...

<br>

To see if we are "seeing pictures in clouds" or there really is a
spatial pattern, we can look at a Moran's scatterplot with a queen's
spatial matrix. The test confirms highly significant autocorrelation.
Our p-values and regression model coefficients cannot be trusted.

<br>

```{r}
spatial.matrix.queen <-poly2nb(Chicago.Census.sf, queen=T)

weights.queen <- nb2listw(spatial.matrix.queen, style='B',zero.policy=TRUE)

moran.plot(Chicago.Census.sf$Fit1.Residuals, weights.queen,
           xlab = "Model 1 residuals",
           ylab = "Neighbors residuals",zero.policy=TRUE)
```

```{r}
moran.test(Chicago.Census.sf$Fit1.Residuals, weights.queen,zero.policy=TRUE)
```

We have a problem! Our residuals definitely do not look like independent
random noise - in fact there is a HUGE cluster of high residuals near
the centre and other residuals around the edge. They also look highly
significant.

The real life interpretation is that in downtown Chicago, the model is
underestimating the percentage of people making more than \$75K - and in
the suburbs it is underestimating.

Let's try a better model:

<br> <br>

### Model 2: Multiple regression

Our model isn't very good. Let's try multiple predictors - so this is
Multiple Linear Regression. I'm going to hope that my spatial dependence
can be removed by including some confounding variables.

<br>

So, now lets see if adding a second predictor variable makes the fit
better. I'm guessing that census tracts where there are more people
earning over \$75K might also be more likely to own their own home.

```{r}
# Make an interactive plot
# http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually

p <- Chicago.Census.sf %>%                  
  ggplot( aes(pop.densityE,per.income.gt75E,col= per.owneroccupiedE,label=NAME)) +
  geom_point() +
  theme_classic()+
  scale_color_gradient(low="blue", high="red")

ggplotly(p)
```

<br>

There seems to be some relationship. Let's ADD this to the model and
take a look. We now explain a little more..

```{r}
# using the OLS package
fit2.lm <- lm(per.income.gt75E ~ pop.densityE + per.owneroccupiedE, data = Chicago.Census.sf)
```

We can see the model equation here, it's harder to see on a scatterplot

```{r}
fit2.lm
```

```{r, include=FALSE, results=FALSE}
equatiomatic::extract_eq(fit2.lm,use_coefs=TRUE)
```

So from the above, our model is now:<br>

```{r, L7Eqn2, echo=FALSE}
knitr::include_graphics('./index_images/pg_364Lab7_Regression_2021_eqn2.png')
```

<br> <br>

#### Is model 2 any good?

Look at the summary

```{r}
ols_regress(fit2.lm)
```

and examine our prediction

```{r}
Chicago.Census.sf$Fit2.lm.Prediction <- predict(fit2.lm)

# subset the Illinois census data with the Chicago city limits
maplm2 <- tm_shape(Chicago.Census.sf, unit = "mi") + 
  tm_polygons(col = "Fit2.lm.Prediction", style = "quantile",
              palette = "-Spectral", border.alpha = 0, 
              title = "MODEL2 prediction: %ppl making > $75K")+
  tm_shape(Chicago.city.sf) +
  tm_borders()+
  tm_layout(legend.outside = TRUE) 
```

```{r, message=FALSE,warning=FALSE}
map_actual

maplm2
```

Also, the model improved! A little! We can see that the R^2^ went up to
0.188. But it's still pretty rubbish.

<br><br>

#### Looking for Spatial autocorrelation in Model 2 residuals

How about the spatial autocorrelation?

To do, this we first add the new residuals to our table

```{r}
Chicago.Census.sf$Fit2.Residuals <- residuals(fit2.lm)
```

Now let's have a look! If we have fully explained all the data and the
data is spatially independent then there should be no pattern.

We can look at the residuals directly:

```{r}
# subset the Illinois census data with the Chicago city limits
tm_shape(Chicago.Census.sf, unit = "mi") + 
           tm_polygons(col = "Fit2.Residuals", style = "quantile",
                       palette = "-RdBu", border.alpha = 0, 
                       title = "Fit 2 residuals: Raw values")+
    tm_shape(Chicago.city.sf) +
           tm_borders()+
    tm_layout(legend.outside = TRUE) 
```

Or.. we can look at the extreme residuals by converting to standard
deviation.

```{r}
Chicago.Census.sf$sd_breaks <- scale(Chicago.Census.sf$Fit2.Residuals)[,1]
# because scale is made for matrices, we just need to get the first column using [,1]

my_breaks <- c(-14,-3,-2,-1,1,2,3,14)

tm_shape(Chicago.Census.sf) + 
  tm_fill("sd_breaks", title = "Fit2 Residuals (StandardDev)", style = "fixed", breaks = my_breaks, palette = "-RdBu") +
  tm_borders(alpha = 0.1) +
  tm_layout(main.title = "Fit 2 Residuals (Standard Deviation away from 0)", main.title.size = 0.7 ,
            legend.position = c("right", "bottom"), legend.title.size = 0.8)+
      tm_layout(legend.outside = TRUE) 

```

<br>

We still have a problem! It is definitely not independent random noise -
there are still HUGE clusters of high residuals near the centre and
other residuals around the edge. E.g. in downtown Chicago, the model is
underestimating the percentage of people making more than \$75K - and in
the suburbs it is underestimating.

To see if we are "seeing pictures in clouds" or there really is a
spatial pattern, we can look at a Moran's scatterplot with a queen's
spatial matrix. The test confirms highly significant autocorrelation.
Our p-values and regression model coefficients cannot be trusted.

<br>

```{r}
spatial.matrix.queen <-poly2nb(Chicago.Census.sf, queen=T)

weights.queen <- nb2listw(spatial.matrix.queen, style='B',zero.policy=TRUE)

moran.plot(Chicago.Census.sf$Fit2.Residuals, weights.queen,
           xlab = "Model residuals",
           ylab = "Neighbors residuals",zero.policy=TRUE)
```

```{r}
moran.test(Chicago.Census.sf$Fit2.Residuals, weights.queen,zero.policy=TRUE)
```

So there is still highly significant spatial autocorrelation.

### Comparing models 1 and 2

How do we know if the "improvement" we saw from model 2 is enough to be
significant or meaningful?

One way to check is to compare the two models using ANOVA. This conducts
a hypothesis test to assess whether there is additional value to adding
a new variable.

```{r}
anova(fit1.lm,fit2.lm)
```

In this case, the p-value is very low - so it would be very unusual to
see this much benefit by chance if per.owneroccupiedE wasn't useful.
E.g, it might be good to keep per.owneroccupiedE.

<br>

We can also use AIC a non parametric statistic, where the LOWER number
is typically the better fit (taking into account over-fitting). In this
case, we can see that it thinks per.owneroccupiedE is a useful variable
to keep even if it has limited influence.

```{r}
AIC(fit1.lm,fit2.lm)
```

<br> <br>

### Model 3: Spatial lag model - OPTIONAL, here for information and your projects

Both models 1 and 2 have not been spatially independent and really
should not be used as our final prediction.

<br>

If the Moran's test is significant (as in this case), then we possibly
need to think of a more suitable model to represent our data: a spatial
regression model. Remember spatial dependence means that (more
typically) there will be areas of spatial clustering for the residuals
in our regression model. We want a better model that does not display
any spatial clustering in the residuals.

<br>

There are two general ways of incorporating spatial dependence in a
regression model:

-   A spatial error model
-   A spatial lagged model

<br>

The difference between these two models is both technical and
conceptual. The spatial error model assumes that the:

*“Spatial dependence observed in our data does not reflect a truly
spatial process, but merely the geographical clustering of the sources
of the behavior of interest. For example, citizens in adjoining
neighborhoods may favour the same (political) candidate not because they
talk to their neighbors, but because citizens with similar incomes tend
to cluster geographically, and income also predicts vote choice. Such
spatial dependence can be termed attributional dependence” (Darmofal,
2015: 4)*

<br>

The spatially lagged model, on the other hand, incorporates spatial
dependence explicitly by adding a “spatially lagged” variable y on the
right hand side of our regression equation. It assumes that spatial
processes THEMSELVES are an important thing to model:

*“If behavior is likely to be highly social in nature, and understanding
the interactions between interdependent units is critical to
understanding the behavior in question. For example, citizens may
discuss politics across adjoining neighbors such that an increase in
support for a candidate in one neighborhood directly leads to an
increase in support for the candidate in adjoining neighborhoods”
(Darmofal, 2015: 4)*

<br>

Mathematically, it makes sense to run both models and see which fits
best. We can do this using the `lm.LMtests()` function. (note, we are
skipping over complexity here!). See here for more details on the full
process:
<https://maczokni.github.io/crimemapping_textbook_bookdown/spatial-regression-models.html>

<br>

But that goes beyond the scope of this course. Here we will try the
spatial lag model, because I can imagine that things like broadband
access have explicit spatial relationships (e.g. where the cable goes)

To fit a spatial lag model, we use

```{r}
fit_2_lag <- lagsarlm(per.income.gt75E ~  pop.densityE + per.owneroccupiedE, data = Chicago.Census.sf, weights.queen,zero.policy=TRUE)
fit_2_lag
```

<br>

This is now going beyond the scope of this course, instead of a simple
linear model, we are running a generalized additive model (GAM), which
is mathematically more complex:

per.income.gt75 = rho(SPATIAL.WEIGHTS\*per.income.gt75) + b0 +
b1xpop.densityE + b2xper.owneroccupiedE e.g.

per.income.gt75 = 0.11255(SPATIAL.WEIGHTS\*per.income.gt75) + -0.0358 +
0.0000026054xpop.densityE + 0.121449xper.owneroccupiedE

<br>

You will notice that there is a new term Rho. What is this? This is our
spatial lag. It is a variable that measures the percentage working from
home in the census tracts SURROUNDING each tract of interest in our
spatial weight matrix.

<br>

We are simply using this variable as an additional explanatory variable
to our model, so that we can appropriately take into account the spatial
clustering detected by our Moran’s I test. You will notice that the
estimated coefficient for this term is both positive and statistically
significant. In other words, when the percentage working from home in
surrounding areas increases, so does the percentage working from home in
each country, even when we adjust for the other explanatory variables in
our model.

<br> <br>

#### Compare model fit

Let's use AIC to compare all 3 models (ANOVA is inappropriate for a GAM)

```{r}
AIC(fit1.lm,fit2.lm,fit_2_lag)
```

We see that our new lagged version has the lowest AIC and so is likely
to be the best model (so far) for predicting the percentage of people
who work from home in each census tract.

<br>

#### Check autocorrelation

Now, if we have fully taken into account the spatial autocorrelation of
our data, the spatial residuals should show less pattern and less
autocorrelation. Let's take a look,

<br>

```{r}
# Create the residuals
Chicago.Census.sf$Fit2.LaggedResiduals <- residuals(fit_2_lag)

Chicago.Census.sf$sd_breaks.lagged <- scale(Chicago.Census.sf$Fit2.LaggedResiduals)[,1]
# because scale is made for matrices, we just need to get the first column using [,1]

#plot standard deviations
my_breaks <- c(-14,-3,-2,-1,1,2,3,14)
tm_shape(Chicago.Census.sf) + 
  tm_fill("sd_breaks", title = "Fit2Lag Residuals (StandardDev)", style = "fixed", breaks = my_breaks, palette = "-RdBu",midpoint=0) +
  tm_borders(alpha = 0.1) +
  tm_layout(main.title = "Residuals for lagged model (Standard Deviation away from 0)", main.title.size = 0.7 ,
            legend.position = c("right", "bottom"), legend.title.size = 0.8)+
      tm_layout(legend.outside = TRUE)
```

```{r}
#plot Moran's I
moran.plot(Chicago.Census.sf$Fit2.LaggedResiduals, weights.queen,
           xlab = "Model residuals",
           ylab = "Neighbors residuals",zero.policy=TRUE)
```

```{r}
moran.test(Chicago.Census.sf$Fit2.LaggedResiduals, weights.queen,zero.policy=TRUE)


```

Well... we have improvement, but there is still significant spatial
variability left in the model. This suggests that I guessed completely
the wrong process for what influences the percentage of people making
over \$75K in a census tract. In this case, I would go away and think
then come back with a new model (amount of green space? proximity to
schools or the metro?).

We could also consider adjusting our spatial weights matrix (maybe a
"neighbor" is a 2nd order queens, or the census tracks within 50km..).

But even so, we have successfully included location in our model.

<br>

#### Plotting the results

So we have a model!

Finally, we went to all the trouble to create a model predicting the
percentage of people who make more than \$75K - let's look at all the
results together:

```{r}
Chicago.Census.sf$Fit2.Lagged.Prediction <- predict(fit_2_lag)

# subset the Illinois census data with the Chicago city limits
maplm2LAG <- tm_shape(Chicago.Census.sf, unit = "mi") + 
  tm_polygons(col = "Fit2.Lagged.Prediction", style = "quantile",
              palette = "-Spectral", border.alpha = 0, 
              title = "MODEL2LAG prediction: %ppl making > $75K")+
  tm_shape(Chicago.city.sf) +
  tm_borders()+
  tm_layout(legend.outside = TRUE) 
```

```{r, message=FALSE,warning=FALSE}

tmap_arrange(map_actual,maplm1,maplm2,maplm2LAG)

```

Still not great, but better.... and we could continue to improve given
that this model is missing something that pushes down numbers in the
suburbs (maybe distance to facilities?).

<br>

This process is the centre of an entire course I teach on regression if
anyone is interested in the topic:
<https://online.stat.psu.edu/stat462/>

## Challenge D2:

8.  **Step 8:** <br> OPTIONAL. Make a new sub-heading called Regression.
    Below, answer these questions. As in the previous challenges, make
    sure they are clearly marked to make it easy to find and grade them.
    NOTE these will be covered in later lectures (see here also: )

<!-- -->

a.  What are the 4 assumptions underpinning linear regression?
    (<https://www.statology.org/linear-regression-assumptions/>)<br>
b.  If the data is spatially auto-correlated, which assumption is
    broken?<br>
c.  What is the ecological fallacy and why is it important when looking
    at census data <br>
d.  If I tested 3 models using AIC and saw the values (Model1: -2403,
    Model2: -2746, Model3:-3102), which model would I be likely to
    choose? <br>

<br>

9.  **Step 9:** <br> Now for your city. Conduct a SIMPLE LINEAR
    REGRESSION analysis to assess whether the percentage of people who
    work from home is linked to the percentage of people who have access
    to broadband. <br> Make sure to tell me what you are doing at each
    stage and to interpret your output in text. Note, we will also be
    going through this in lectures.

<br>

10. **Step 10:** <br> Now for your city. Conduct a MULTIPLE LINEAR
    REGRESSION analysis to assess whether the percentage of people who
    work from home is linked to the percentage of people who have access
    to broadband AND some other variable you think might influence it.
    <br> Make sure to tell me what you are doing at each stage and to
    interpret your output in text.

<br>

11. **Step 11:** <br> Compare models 1 and 2 using the techniques in the
    tutorial

<br>

12. **Step 12:** <br> The American Community Survey data was collected
    in 2017. Do you think these results are stable now? If not, why not?

<br>

13. **Step 13:** <br> OPTIONAL.<br> For 4 show me something new points,
    go further. Apply and interpret either/or/both multiple regression
    (e.g. two or more things that might influence the percentage of
    people who work from home), AND/OR apply and interpret a spatial lag
    model. I will also consider exceptional analyses for course credit.

<br>

## E. Above and beyond

Remember that an A is 93%, so you can ignore this section and still
easily get an A. But here is your time to shine. Also, if you are
struggling in another part of the lab, you can use this to gain back
points.

**To get the final 4 marks in the lab, you need to show me something
new, e.g. you need to go above and beyond the lab questions in some
way.**

-   You get 2/4 for doing something new in any way
-   You get 4/4 for something really impressive (see the two ideas in
    the steps)

Please tell us in your R script what you did!

<br><br>

## F. Submitting your Lab

Remember to save your work throughout and to spell check your writing
(left of the knit button). Now, press the knit button again. If you have
not made any mistakes in the code then R should create a html file in
your lab 6 folder which includes your answers. If you look at your lab 6
folder, you should see this there - complete with a very recent
time-stamp.

In that folder, double click on the html file. This will open it in your
browser. CHECK THAT THIS IS WHAT YOU WANT TO SUBMIT

Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 6.

<br><br>

## Lab 6 submission check-list

**For all answers: Full marks = everything down at a high standard, in
full sentences as appropriate with no parts of your answer missing.
Imagine it as an example I use in class**

**HTML FILE SUBMISSION - 5 marks**

**RMD CODE SUBMISSION - 5 marks**

**MARKDOWN STYLE - 10 MARKS**

We will start by awarding full marks and dock marks for mistakes.LOOK AT
YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT

TO GET 13/13 : All the below PLUS your equations use the equation format
& you use subscripts/superscript as appropriate

TO GET 12/13 - all the below:

-   Your document is neat and easy to read.
-   Answers are easy to find and paragraphs are clear (e.g. you left
    spaces between lines)
-   You have written in full sentences, it is clear what your answers
    are referring to.
-   You have used the spell check.
-   You have used YAML code to make your work look professional (themes,
    tables of contents etc)

**Above and beyond: 4 MARKS**

-   You get 2/4 for doing something new in any way
-   You get 4/4 for something really impressive

An easy example, try conducting a Moran's analysis for your census data.
Bonus marks if you can convert a column to TRUE/FALSE and do a join
counts analysis on it! [100 marks total]

Overall, here is what your lab should correspond to:

```{r, echo=FALSE}
rubric <- readxl::read_excel("pg_364Lab_rubrictable.xlsx")
knitr::kable(rubric) %>%   
  kable_classic_2() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```

::: {style="margin-bottom:25px;"}
:::

------------------------------------------------------------------------

Website created and maintained by [Helen
Greatrex](https://www.geog.psu.edu/directory/helen-greatrex). Website
template by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)
