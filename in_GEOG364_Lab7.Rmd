---
title: "Lab 7: Point Pattern Analysis"
output:
  html_document:
    css: projectlab.css
    highlight: pygments
    theme: flatly
    toc: true
    toc_float: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: sentence
---

```{r, include=FALSE,echo=FALSE, warning=FALSE, message=FALSE}
# NOTES
```

```{r, include=FALSE,echo=FALSE, warning=FALSE, message=FALSE}

knitr::opts_chunk$set(eval = TRUE, 
                      fig.show = "hide", 
                      message = FALSE, 
                      warning = FALSE)

# Libraries
library(AER)
library(broom)
library(elevatr)
library(ggplot2)
library(kableExtra)
library(openintro)
library(osmdata)
library(sp)
library(sf)
library(plotly)
library(RColorBrewer)
library(readxl)
library(rmapshaper)
library(skimr)
library(sfdep)
library(spatstat)
library(spdep)
library(tidycensus)
library(tidyverse)
library(tigris)
library(tmap)
library(viridis)


# Data
rubric <- read_excel("./index_data/Table_LabRubric.xlsx")

```

------------------------------------------------------------------------

\

# Welcome to Lab 7!

**Aim of the lab**

The analysis of point patterns is focused on the spatial distribution of observed events and making inference about the underlying process that generated them. We have two main issues of interest: the distribution of events in space and the existence of possible interactions between them


::: {#boxgreen}
**This is a TWO WEEK LAB** See [here](https://psu.instructure.com/courses/2396422/assignments/17585714) for assignment guidelines.
You must submit an `.Rmd` file and its associated `.html` file.
:::

\

::: {#boxedtext}
\
**Getting help**\
- Kiely (and often Dr G) will be present during your lab sessions.
This is the easiest way to get help.\
- Dr G has weekly office hours and genuinely enjoys helping with R, even if you feel stuck or overwhelmed.\
- You may send a Canvas message to Kiely or Dr G (or if you are completely lost).\

\
:::

\

------------------------------------------------------------------------

\

# Set-up. DON'T SKIP!

\

### Create a project & get the data

There are two options here depending on whether you are using R-studio on the website (posit cloud) or your own computer (R-Desktop).
If you are using a lab computer choose the R-Desktop route.

#### Option 1. Posit Cloud Website Users

<details>

<summary>Task 1.
Create a project for Lab 7 (expand for instructions)</summary>

::: collapsible-content
<iframe src="https://psu-spatial.github.io/Stat462-2025/T5_ProjectsPositCloud.html" style="width: 100%; height: 700px; border: none;">

</iframe>
:::

<br>

</details>

::: small-gap
:::

<details>

<summary>Task 2.
Install more packages (expand for instructions)</summary>

<br>

Unfortunately on the website you need to install your packages each time.<br>


First we need a package called sf, which runs a lot of the spatial commands in R.
Unfortunately, posit cloud sometimes has a few technical issues with sf, so you will need to run a special command.

IN THE CONSOLE, run these two commands.

```{r,eval=FALSE}
install.packages("remotes")
remotes::install_github(repo = "r-spatial/sf", ref = "93a25fd8e2f5c6af7c080f92141cb2b765a04a84")
```


Go to to the packages tab, click install to get to the app-store and download/install these packages:

-   `AER`
-   `broom`
-   `elevatr`
-   `ggplot2`
-   `kableExtra`
-   `openintro`
-   `osmdata`
-   `sp`
-   `sf`
-   `plotly`
-   `RColorBrewer`
-   `readxl`
-   `rmapshaper`
-   `skimr`
-   `sfdep`
-   `spatstat`
-   `spdep`
-   `tidycensus`
-   `tidyverse`
-   `tigris`
-   `tmap`
-   `viridis`



T6_Packages.html <br>*Reminder: [Tutorial: Packages cheatsheet](https://psu-spatial.github.io/Stat462-2025/T6_Packages.html)*.

</details>

#### Option 2. R-Desktop Users

<details>

<summary>Task 1.
Create a project for Lab 7</summary>

<br>

::: collapsible-content
<iframe src="https://psu-spatial.github.io/Stat462-2025/T5_ProjectsRDesktop.html" style="width: 100%; height: 700px; border: none;">

</iframe>
:::

</details>

::: small-gap
:::

<br>

### Set-up your Lab 7 report

You are welcome to use/edit the template you made in previous labs.
If you are unsure what I mean by that, follow these instructions.

<details>

<summary>Task.
Create your RMarkdown file - expand & look at Tutorial 4B and 4C</summary>

<br>

::: collapsible-content
<iframe src="https://hgreatrex.github.io/Geog364_2024/pg_Tut4_markdown.html#Tutorial_4B:_Creating_a_markdown_document" style="width: 100%; height: 700px; border: none;">

</iframe>
:::

</details>

::: small-gap
:::

<details>

<summary>Task.
Edit your YAML code & ADD A NEW THEME</summary>

<br>

Lets use similar options to Lab 4.
Remember YAML code is annoying to edit, because here, *spaces really do matter*.
Everything has to be perfect or it won't knit.

-   **Select everything in my code chunk here and replace your YAML with this (remember the --- on line 1 and at the end).**

-   Now edit the author name to your own.
    If you wonder what Sys.Date() is, don't touch it - it automatically gives you the current date.

-   Now change your theme to your favourite one of these - you can see what it looks like by pressing knit.
    Note, DO NOT put quote marks around the theme name.

    -   bootstrap
    -   cerulean
    -   cosmo
    -   darkly
    -   flatly
    -   journal
    -   lumen
    -   paper
    -   readable
    -   sandstone
    -   simplex
    -   spacelab
    -   united
    -   yeti

```{r,eval=FALSE}

#---------------------------------------------------------
# NOTE, Your theme does NOT have quote marks around it 
#---------------------------------------------------------
---
title: "GEOG-364 - Lab 7"
author: "hlg5155"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: yes
    number_sections: yes
    theme: lumen
    df_print: paged
---
  
  
```

</details>

::: small-gap
:::

<details>

<summary>Task.
Delete the existing text and create/run your library code chunk</summary>

<br>

-   Click on your lab script (the Rmd file) and delete all the 'welcome text' after line 11.<br>Press enter a few times and make a new level-1 heading called `Set Up`.<br>

-   We should have all the packages we need installed, but we need to open them.
    Make a new code chunk containing this code.<br>

```{r,message=FALSE,warning=FALSE, eval=FALSE}
rm(list=ls())

library(AER)
library(broom)
library(elevatr)
library(ggplot2)
library(kableExtra)
library(openintro)
library(osmdata)
library(sp)
library(sf)
library(plotly)
library(RColorBrewer)
library(readxl)
library(rmapshaper)
library(skimr)
library(sfdep)
library(spdep)
library(tidycensus)
library(tidyverse)
library(tigris)
library(tmap)
library(viridis)
library(spatstat) # NEW!  You might have to install this.

```

<br>

-   Press the green arrow on the right of the code chunk to run the code inside it.
    You will see a load of "loading text" telling your details about the packages you just loaded.<br><br> Press the green arrow AGAIN.
    The text should disappear unless there is an error.<br>

-   Note, remember to run this code chunk EVERY TIME your start R-Studio (in the same way you need to click on an app on your phone before you can use it).
    <br>

You might need additional libraries as you work through the lab.
If so, add them in this code chunk AND REMEMBER TO RERUN. If you see a little yellow bar at the top asking you to install them,click yes!

<br>

</details>

::: small-gap
:::

<details>

<summary>Task.
Add warning=FALSE and message=FALSE to your library code chunk.</summary>

<br><br><br>


::: collapsible-content
<iframe src="https://psu-spatial.github.io/Stat462-2025/T7_Markdown_4bCodeChunkOptions.html" style="width: 100%; height: 700px; border: none;">

</iframe>
:::

</details>

::: small-gap
:::

### Check your progress

Your lab script should now look similar this, but with your theme and YAML options of choice (you might have a few different libraries than in my screenshot).
You should also be able to knit it successfully.
If not, go back and do the previous sections!

<br>

![](index_images/pg_364Lab2_Summary_2021_fig2.png)

<br><br>


------------------------------------------------------------------------


# WEEK 1

## A. Introduction

This lab introduces **point pattern analysis**, a technique used to study the spatial distribution of individual events or locations. We'll explore how and where things happen by examining the locations of **McDonald’s restaurants** and **schools** in **Upstate New York**. By comparing their spatial patterns, we’ll begin to ask: *Are these points randomly distributed, clustered, or evenly spaced at different scales.* Then use our results to explore the social, economic, and geographic processes that shape them.

Point pattern analysis examines how individual events or features, such as shops, disease locations, or trees are arranged in space. Unlike earlier labs that used fixed areas like counties, this method does not rely on predefined boundaries. Instead, Point pattern analysis is concerned with describing patterns of points over space and making inference about the process that could have generated an observed pattern. The techniques are widely used in fields like public health, urban planning, and environmental science.

-   This is a great overview, <https://r-spatial.org/book/11-PointPattern.html>

<br><br>

### Report style - important!

You will be graded on your report style. To get an A grade, submit a clean, well-organized report. You should use headings and sub-headings, keep code output tidy, make sure all units are included and write clear explanations beneath each task.

Your report should make it easy for the reader to locate each answer and understand how your results relate to real-world questions. Only polished, professional submissions will receive 100%.

<br><br>

### The spatstat package

In this lab you will begin working with the R package spatstat, which is specifically designed for spatial point-pattern analysis. Although its interface can feel somewhat old fashioned and it hates sf/terra, it remains one of the most comprehensive environments for working with spatial point processes. The package is supported by a large body of documentation, including a dedicated textbook and an active set of online resources. <br>

You will learn how to prepare your data so that it can be used effectively for spatial analyses by converting it into the appropriate format used by spatstat. This initial exercise will give you a conceptual foundation before we start manipulating data. 

<br><br>

### Tasks

1.  Explain the basics

-   Remembering to use headings/sub-headings so it's easy for us to find your answer, describe what point pattern is - and link to one resource online that helps describe point pattern analysis other than the spatstat and R-spatial books.

-   Visit the spatstat FAQ page at <https://spatstat.org/FAQ.html> and explore the content.<br>

-   Remembering to use headings and sub-headings, write (at minimum) 50 words describing what spatstat is and any features or aspects of the package that seem especially useful or interesting.<br>

<br><br>

## **B. Set up your case study.**

This lab focuses on understanding the spatial patterns of McDonald’s restaurants and high schools across Upstate New York. First we need to get the data<br>

2.  Loading the data

-   Download `Lab6_SpatialData_NY.RData` from canvavs and put them into your Lab 7 folder.<br><br>

-   A .RData file stores a collection of R objects so that you can load several datasets at once. Add a new code chunk and use the load() function to bring the data into your session. 

```{r}
load("Lab7_SpatialPoint_NY.RData")
```



```{r, include=FALSE, warning=FALSE, message=FALSE, output="hide", eval=FALSE}


Schools <- st_transform(Schools,crs=4326)
cropbox <- st_bbox(Schools)
cropbox["ymin"] <- 41.956900
cropboxutm <- st_transform(cropbox,crs=st_crs(McDonalds))
Schools <- st_crop(Schools, cropbox)|> st_transform(crs=st_crs(McDonalds))
McDonalds <- st_transform(McDonalds,crs=4326)
McDonalds <- st_crop(McDonalds, cropbox)|> st_transform(crs=st_crs(Schools))


library(spatstat.geom)
crop_window <- owin(xrange = c(597530, 1276300), yrange = c(4645947, 5025500) )
median_income <- median_income[crop_window]
pop_density <- pop_density[crop_window]
rm(crop_window)
rm(cropbox)


ymin_new <- 4645947

crop_poly <- st_polygon(list(rbind(
  c(597530,    ymin_new),
  c(1276300,   ymin_new),
  c(1276300,   5025500),
  c(597530,    5025500),
  c(597530,    ymin_new)
))) |> 
  st_sfc(crs = 32617)
stateborder <- st_intersection(stateborder.utm, crop_poly)
plot(st_geometry(stateborder.utm), col = "lightgray")
plot(st_geometry(crop_poly), add = TRUE, border = "red")
plot(st_geometry(stateborder),  col = "blue")
rm(cropboxutm)
rm(crop_poly)
rm(stateborder.utm)
rm(ymin_new)

save("Lab7_SpatialPoint_NY.RData")
save.image("Lab7_SpatialPoint_NY.RData")


# Background code
oldmedincome <- median_income
median_income <- NYmedian_income
oldowin <- stateborder.utm
stateborder.utm <- NYstateborder.utm
NYmedincome <- median_income
median_income <- oldmedincome

```


-   Run BOTH your library code chunk and this one, then click on your environment tab, you should see that it has downloaded several datasets about New York. <br><br>

-   Look at the Environment tab and you'll see several datasets have appeared:

    -   `median_income` and `pop_density` are derived from census tract vector data from the American Community Survey. Rather than our normal vector format, they are saved as *raster images* (to make the point pattern code work). <br>

    -   `Schools` is a point dataset containing the location of all the high-schools in Upstate New York State as of 2022-2023. I obtained it from the New York State GIS portal<br>

    -   `McDonalds` is a dataset containing the location of all the McDonalds resturants in Upstate New York State. I obtained it from kaggle and I don't know the original source <br>

**THIS WILL ONLY WORK IF YOU HAVE RUN THE LOAD LIBRARY CODE CHUNK TO LOAD THE SPATSTAT PACKAGE. If you see a list of stuff and it doesn't make a map, install and load library(spatstat)**

<br><br> 
    



## **C. Data Exploration**

3.  Mapping your data

Before working with point-pattern tools, it is useful to take a first look at how the Schools and McDonald’s locations are distributed across New York. The example below creates an interactive map of the school locations using tmap in view mode:

```{r, eval=FALSE}
tmap_mode("view")

tm_shape(Schools) +
  tm_dots(size = 0.5, fill_alpha = 0.5, fill = "blue") +
  tm_basemap("OpenStreetMap") +
  tm_credits("High Schools in New York")

```

<br>

-   Get this map working in your report, then create a similar map for the McDonald’s dataset. You are encouraged to adjust the style: experiment with point size, colour, basemaps, or additional layers such as the New York State boundary (`stateborder.utm`). For basemap ideas, see the preview gallery at <https://leaflet-extras.github.io/leaflet-providers/preview/>.

<br><br>

4.  Explain the patterns & likely process causing them

In your report, underneath the plots begin by briefly describing what each dataset represents. Then compare the spatial distributions of Schools and McDonald’s locations across New York State. Draw on the examples from Lectures 7–8 and Homework 3 to structure your interpretation.:<br>

-   Describe whether each dataset appears to show positive, zero, or negative spatial autocorrelation. Comment on whether this conclusion changes as you adjust the spatial domain/scale of observation (for example, when zooming in to a city versus viewing the entire state). <br>

-   Explain the possible spatial processes that may be generating these patterns. As a hint, if you were asked to predict where new Schools or McDonald’s locations might appear in another state, what variables or contextual information would you consider?:<br>

-   Discuss whether each observed patterns seems driven by first-order processes, second-order processes, or a combination of both - and explain what you mean by these terms.  Hint, see https://mgimond.github.io/Spatial/chp11_0.html:<br>

-   Discuss whether the interpretation of either dataset may be affected by each of these spatial issues, explaining your reasoning for each.

    -   the locational fallacy:<br>

    -   non-uniformity of space:<br>

    -   the Modifiable Areal Unit Problem (MAUP)<br><br>:<br>

<br><br>


5.  Find the map projection

If you type Schools into the console and press enter, you should see in the summary that your data's coordinates are not lat/long.  It's good practice to know what map projection your data is in. 

 - To do this, you can use the `st_crs()` command, e.g.try this

```{r,eval=FALSE}
st_crs(Schools)
```

 - At the top of the output you should see the EPSG code. <br><br>
 - Google/chatgpt the code and in your report, explain what the map projection is, why I chose it and what units it is in. 


<br><br>  



## **D.Convert the data into 'spatstat' format**

### Converting data to the ppp format

All of the point pattern tools used in this section come from the spatstat package. These commands work with points stored in ppp objects rather than the sf objects you have used so far. Once we move into spatstat, packages such as tmap will no longer work as expected.

<br>

6. Creating a window for the study area

A key requirement of spatstat is that each point pattern must be associated with a clearly defined spatial window to show the boundary of the point data. <br><br>

-   We begin by converting the New York State boundary into an owin object. This defines the spatial window for our analysis so that the point pattern is evaluated within the state boundary rather than within a simple rectangular extent. Run the code below in your own lab setup. It creates the window object we will use throughout:<br>

```{r}
stateborder.owin <- as.owin(stateborder)
```


<br><br>


7. Make your data a ppp (point pattern object)

Now we need to convert our data into a ppp object.

-  For this first step we will work with unmarked point data. For both the Schools and McDonalds datasets, extract only the geometry (the X and Y coordinates), convert each to a ppp object, and then assign the New York window you created above. Here is the code for Schools. Get it working in your report and also make one for McDonalds
 
```{r}
# Just take XY locations
School.locs  <- st_geometry(Schools)

# Convert to point pattern format
School.ppp  <- as.ppp(School.locs)

# Set the domain to be NY State
Window(School.ppp) <- stateborder.owin

School.ppp

```


<br><br>

8. Make your data a ppp (point pattern object)

-   To make a good looking map in spatstat, we need to play with the plot margins. You can see how I did that here for the schools data. Get this working in your code and also make a map of the McDonalds data.


```{r}
# Save current plot settings
oldpar <- par(no.readonly = TRUE)

# Make temporary changes to make the margin smaller
par(oma = c(0,0,0,0),
    mar = c(.5,.5,1,.5),
    cex.main = .9)

# Produce your plot
plot(School.ppp,
     pch = 16,
     cols = rgb(0,0,0,.5),
     cex = .5,
     main = "Point pattern of NY schools data")

# Restore previous plot settings
par(oldpar)
```

<br><br>


9. Convert units to kn.

All of our objects are currently measured in metres. Because we are working at the scale of an entire state, reporting results in metres is unwieldy. We will therefore convert every dataset to kilometres.

-   Run the code below in your report for schools. **If you encounter errors, tell Kiely or Dr G.** If it really doesn't work, skip the conversion, re-run your code from the start, and continue the lab using metres. <br>

```{r}
# First reload the spatstat geom package.  You don't need to move it to the top as I want to force it to be the final package loaded
library(spatstat.geom)

School.ppp    <- spatstat.geom::rescale(School.ppp, 1000, "km")
median_income <- spatstat.geom::rescale(median_income, 1000, "km")
pop_density   <- spatstat.geom::rescale(pop_density, 1000, "km")

```

 - If that worked, also convert the units of your McDonalds data to km.
 
 


<br><br>


## **E Modelling first order effects**

In spatial point pattern analysis, first-order effects refer to variations in the intensity of points across a study area due to external factors. These effects describe how the likelihood of observing a point changes from one location to another, independent of the presence or absence of other points. For example, oak trees may be more densely distributed in areas with favorable soil conditions, or retail stores may cluster in regions with higher population density. In both cases, the spatial variation is driven by underlying environmental or socioeconomic covariates rather than interactions between the points themselves.

**To explore first order effects, we look at the density of points across our study area and see how that compares to point patterns made by different processes.**


https://mgimond.github.io/Spatial/chp14_0.html#understanding-intensity-and-density-in-point-patterns

<br><br>

10. Explain the background

-   Make a new level one heading called First order effects. 

-   One piece of jargon to understand is the difference between intensity and density. In your report, using this tutorial, https://mgimond.github.io/Spatial/chp14_0.html#understanding-intensity-and-density-in-point-patterns , explain the difference in your own words.


<br><br>


### **F. Global Density**


A simple summary of a point pattern's intensity is its global density: the number of points per unit area. For example, we might report penguins per square mile or flowers per square foot. The formula is simple: *global density = number of points / area of the study region*.

But we are assuming that the pattern is homogeneous, e.g. you don't have clusters or hotspots of points that vary across your study domain.  Here is how to compute this quantity and write it up for the schools dataset:

```{r}
# Number of points
num_points <- npoints(School.ppp)
num_points

# Area of the study region
study_area <- area.owin(Window(School.ppp))
study_area

# Global density
global_density <- num_points / study_area
global_density

```

So this code suggests that in New York State, there are 0.0072 schools per square km. E.g. if the schools were evenly distributed in a grid-like pattern, the average distance between schools would be around 12 kilometers in any direction.

**How did I get this?:**  1/0.0072 ~ 139, this means if schools were evenly distributed in a grid like pattern then there would be roughly one school every 139 square kilometers. Taking the square root of 139 gives about 11.79, so if the schools were evenly distributed in a grid-like pattern, the average distance between schools would be approximately 12 kilometers in each direction.

<br><br>


11. Repeat for McDonalds

-   Calculate and interpret the global density for the McDonalds data. Is it greater or lesser than the schools intensity. 

-   (A* bonus question): How does the non-uniformity of space impact this result?


<br><br>

### **G Local density: Kernel Smoothing**

A second way to explore first-order properties is to examine how density varies **locally** across the study area. One common approach is **kernel density estimation (KDE)**, which produces a continuous surface showing where points are more or less concentrated.

The basic idea is straightforward: imagine sliding a circular window across the map and counting how many points fall inside that window at each location. The result is a smoothed surface, which we can use as our best guess of the true underlying intensity. 

![](index_images/pg_Lab6_Kernelexplanation.png)

<br>

```{r, out.width="100%", fig.cap="Illustration of a moving kernel window used to estimate local density."}
knitr::include_graphics("index_images/pg_Lab6_KernelBandwidthEsri2.png")
```


<br><br>

12. Kernel density using the default bandwidth

-   Use the code below to calculate a kernel density surface for the schools data using the default bandwidth. Once it works, copy and adapt the code to create a second version for the McDonald’s dataset. <br>

-   Compare the Schools and McDonald’s maps in your own words. What similarities and differences do you see?  <br> 

```{r, results="hide"}
SchoolDensity1 <- density(School.ppp)   # Default bandwidth
plot(SchoolDensity1, main = NULL, las = 1)
contour(SchoolDensity1, add = TRUE)
```

<br><br>

####  KDE adjusting the bandwidth

Within KDE, you can decide how big you want your smoothing window to be. The width of this window is controlled by the **bandwidth**, which determines how coarse or fine the surface will appear. 

<br>

![](index_images/pg_Lab6_KernelBandwidthEsri.png){width="90%"}

<br>

Different bandwidths tell us different stories about the data. In the example above, the large bandwidth on the left highlights the parts of the city most prone to late-night road accidents but blurs local detail. The small bandwidth on the right reveals specific streets but makes it harder to see the overall pattern.

**There is no single “correct” bandwidth.** Changing it often reveals different aspects of the underlying process. So we mostly use KDE for EXPLORATORY analysis.  

Click here to read more and see some examples in R.
https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#kernel-density-raster

<br><br>

13. Assess how the bandwidth impacts your results.

-   Edit your existing KDE code to include the sigma option (as shown below). For example, if your units are kilometres, sigma = 50 means you are smoothing over a radius of about 50 km, while sigma = 5 focuses on patterns at around 5 km. <br>


```{r, results="hide"}
SchoolDensity <- density(School.ppp, sigma=5) 
plot(SchoolDensity, main=NULL, las=1)
contour(SchoolDensity, add=TRUE)
```

<br>

-   Run the same code several times for both the Schools and McDonald’s data, varying sigma from a very small value (e.g. 1 km) to a very large one (e.g. 1000 km). As you change sigma, think about the real-world processes that might be driving the patterns you see. <br><br>

-   For your report, choose a final bandwidth for each dataset (Schools and McDonald’s) that helps you say something interesting about the pattern in relation to New York’s geography or environment.
Include the corresponding maps and briefly explain your reasoning below each map. <br><br>

-   A* bonus question: Explain how the bandwidth issue links with MAUP's scale effect.<br>


<br><br>

#### WEEK 1 COMPLETE


------------------------------------------------------------------------

# WEEK 2 

## Modelling 2nd order effects

### **H. Nearest neighbour analysis**

The nearest-neighbour distance measures, for each point in a pattern, how far away its closest other point lies. It captures the local spacing between points and is highly sensitive to whether the pattern is clustered, random, or inhibited. When many points have very small nearest-neighbour distances, this indicates clustering; when distances are consistently large, the pattern shows regularity or repulsion. Under complete spatial randomness (CSR), nearest-neighbour distances follow a known theoretical distribution, so departures from it reveal second-order effects—interactions between points that cannot be explained by intensity alone. For this reason, summary functions based on nearest-neighbour distances, such as the L-function, are widely used to diagnose spatial interaction in point pattern analysis.

<br><br>

14. Calculate & plot nearest neighbours

-   The `nndist` command calculates the distance between each point and its nearest neighbour. Because it's just calculating a list of numbers, we can add it as a new column to our dataset and treat it like any other column.

<br>

```{r}

# Use the nndist command, then save the result as a new column in the schools data
School.ppp$NearestNeighbourDist <- nndist(School.ppp)
Schools$NearestNeighbourDist    <- nndist(School.ppp)

```

<br><br>

-   Use the code above to calculate the nearest neighbour distances for the schools data.
    Once it works, copy and adapt the code to create a second version for the McDonald’s dataset.
    <br>

-   For each of your Schools and McDonalds datasets, take the mean of your nearest neighbour column, then use the round command to round it to two decimal places.
    <br>

-   Plot a professional looking histogram of the nearest neighbour column of the Schools data and of the McDonalds datasets.

    -   \*By professional, I mean it's something you would be happy showing in a job interview (E.g. easy to interpret, correct units, labels etc..). For code examples, see my STAT-462 tutorial and scroll down for many different ideas you could tweak: <https://psu-spatial.github.io/Stat462-2024/T6_plots.html#histograms-1>{.uri \_blank=""}\*

-   Under the histograms, the Schools and McDonald’s nearest neighbour distributions in your own words.

    -   What similarities and differences do you see and how does this link to the underlying processes 'causing' schools and McDonalds?

    -   As a hint, your write up should include the words *"Schools in Upstate New York, are on average 8.79km apart."* and similar for your McDonalds data.
        <br>

<br><br>

#### R.Ratio

We can now calculate the R.Ratio and see if it is unusual compared to a pattern caused by an IRP by using a Clarke Evans test.
We will cover this in class on Wednesday, but for now, refer to this reading on Canvas:
https://psu.instructure.com/courses/2396422/assignments/17585717 
(YOU NEED SECTION 14.1.)

<br><br>

15. Explain the R.Ratio

-   In the text explain what the R Ratio is and what you would expect for: <br>

    -   A very clustered point pattern

    -   Complete spatial randomness

    -   A very uniform/gridded point pattern

<br><br>

#### Clarke Evans Hypothesis test

Here is the code for a Clarke Evans hypothesis test to check if our observed schools point pattern is EITHER unusually clustered OR unusually regular/gridded, compared to what we would expect if the pattern was generated by an IRP.

<br>

```{r,eval=FALSE}
 clarkevans.test(School.ppp, alternative="two.sided")
```

<br>

It's also possible to edit this code for a one sided test e.g.

-   \`H1: alternative="clustered"\`: Our observed pattern is unusually clustered compared to a pattern generated by an IRP.
    <br>

-   \`H1: alternative="regular"\`: Our observed pattern is unusually gridded/regular compared to a pattern generated by an IRP.

A full explanation and write up of the test and its interpretation can be found in the Canvas reading:https://psu.instructure.com/courses/2396422/assignments/17585717 
(SECTION 14.1)

<br><br>

16. Run your own Clarke-Evans test

-   Run a Clarke-Evans test to answer this question: "*Are **schools** more regularly spaced (more “gridded”) than we would expect from an Independent Random Process?*" <br>

-   You may choose your own p-value threshold, but you must justify your choice.

-   I want to see your code and a write up.
    Your write up must include:

    -   The null hypothesis (H0)

    -   The alternative hypothesis (H1)

    -   The observed test statistic

    -   The test statistic you would expect if H0 were true

    -   The p-value

    -   A clearly stated conclusion





<br><br>

### **I. L-Function**

TO COME (literally in 5 mins), REFRESH YOUR PAGE

<br><br>

------------------------------------------------------------------------



# Submitting your Lab

Remember to save your work throughout and to spell check your writing (next to the save button).
Now, press the knit button again.
If you have not made any mistakes in the code then R should create a html file in your lab3 folder, complete with a very recent time-stamp.

<br><br>

### If you are on posit cloud:

You can download each of your .RmD and html files by:

-   Clicking on the little box next to the Rmd in the Files tab, then going to the little blue cogwheel (might need to make your Rstudio full screen) and clicking export.<br>

```{r, Lab2FigDownload, echo=FALSE,fig.align='center',out.width="90%"}
knitr::include_graphics('./index_images/im_T2_Download.png')
```

-   Repeat the process exactly for the html file underneath it (e,g, just have the html clicked.)<br>

-   Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 6.

<br>

### Posit desktop

-   Go to your Lab 6 folder, In that folder, double click on the html file.
    This will open it in your browser.
    CHECK THAT THIS IS WHAT YOU WANT TO SUBMIT <br>

-   Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 6.<br>

```{r, echo=FALSE}
knitr::include_graphics("./index_images/pg_364Lab1_Basics_2021_fig1.png")
```

<br><br>




------------------------------------------------------------------------

\

# GRADING RUBRIC

To come

<br><br>

## Overall

Overall, here is what your lab should correspond to:

```{r, echo=FALSE}
knitr::kable(rubric) %>%   
  kable_classic_2() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))

```

And..
finished!